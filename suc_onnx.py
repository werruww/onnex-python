# -*- coding: utf-8 -*-
"""suc_onnx.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hjOdaPqSM5jnXZGnCR5xAciJr2qYXajX
"""





"""https://onnxruntime.ai/docs/get-started/with-python.html"""

!pip install onnxruntime

!pip install onnx

!huggingface-cli download microsoft/Phi-4-mini-instruct-onnx --local-dir ./phi4-mini-onnx

!huggingface-cli download onnx-community/Llama-3.2-3B-Instruct-ONNX --include cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/* --local-dir .

"""https://huggingface.co/onnx-community/Llama-3.2-3B-Instruct-ONNX"""

!pip install --pre onnxruntime-genai

!python model-qa.py -h

"""https://github.com/microsoft/onnxruntime-genai/tree/main

ayhgشغال
"""

import onnxruntime_genai as og

model = og.Model('cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')
tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()

# Set the max length to something sensible by default,
# since otherwise it will be set to the entire context length
search_options = {}
search_options['max_length'] = 2048
search_options['batch_size'] = 1

chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'

text = input("Input: ")
if not text:
   print("Error, input cannot be empty")
   exit

prompt = f'{chat_template.format(input=text)}'

input_tokens = tokenizer.encode(prompt)

params = og.GeneratorParams(model)
params.set_search_options(**search_options)
generator = og.Generator(model, params)

print("Output: ", end='', flush=True)

try:
   generator.append_tokens(input_tokens)
   while not generator.is_done():
     generator.generate_next_token()

     new_token = generator.get_next_tokens()[0]
     print(tokenizer_stream.decode(new_token), end='', flush=True)
except KeyboardInterrupt:
    print("  --control+c pressed, aborting generation--")

print()
del generator

"""https://github.com/microsoft/onnxruntime-genai/tree/main

aشغال جيد
"""

import onnxruntime_genai as og
import os


os.environ["COLAB_DISABLE_DEBUGPY"] = "1"
model = og.Model('cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')
tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()

# Set the max length to something sensible by default,
# since otherwise it will be set to the entire context length
search_options = {}
search_options['max_length'] = 2048
search_options['batch_size'] = 1

chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'

text = input("Input: ")
if not text:
   print("Error, input cannot be empty")
   exit

prompt = f'{chat_template.format(input=text)}'

input_tokens = tokenizer.encode(prompt)

params = og.GeneratorParams(model)
params.set_search_options(**search_options)
generator = og.Generator(model, params)

print("Output: ", end='', flush=True)

try:
   generator.append_tokens(input_tokens)
   while not generator.is_done():
     generator.generate_next_token()

     new_token = generator.get_next_tokens()[0]
     print(tokenizer_stream.decode(new_token), end='', flush=True)
except KeyboardInterrupt:
    print("  --control+c pressed, aborting generation--")

print()
del generator

!python /content/model-qa.py -h

"""ayhgشغال

https://huggingface.co/onnx-community/Llama-3.2-3B-Instruct-ONNX
"""

import onnxruntime_genai as og

model = og.Model('cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')
tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()

search_options = {'max_length': 50, 'batch_size': 1}
chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'

while True:
    text = input("Input (type 'quit' to exit): ")
    if text == "quit":
        break
    if not text:
        print("Error, input cannot be empty")
        continue

    prompt = f'{chat_template.format(input=text)}'
    input_tokens = tokenizer.encode(prompt)

    params = og.GeneratorParams(model)
    params.set_search_options(**search_options)
    generator = og.Generator(model, params)
    generator.append_tokens(input_tokens)

    print("Output: ", end='', flush=True)
    try:
        while not generator.is_done():
            generator.generate_next_token()
            new_token = generator.get_next_tokens()[0]
            print(tokenizer_stream.decode(new_token), end='', flush=True)
    except KeyboardInterrupt:
        print("  --control+c pressed, aborting generation--")
    print()
    del generator

"""https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx

phi4
"""

import onnxruntime_genai as og

model = og.Model('/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')
tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()

search_options = {'max_length': 50, 'batch_size': 1}
chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'

while True:
    text = input("Input (type 'quit' to exit): ")
    if text == "quit":
        break
    if not text:
        print("Error, input cannot be empty")
        continue

    prompt = f'{chat_template.format(input=text)}'
    input_tokens = tokenizer.encode(prompt)

    params = og.GeneratorParams(model)
    params.set_search_options(**search_options)
    generator = og.Generator(model, params)
    generator.append_tokens(input_tokens)

    print("Output: ", end='', flush=True)
    try:
        while not generator.is_done():
            generator.generate_next_token()
            new_token = generator.get_next_tokens()[0]
            print(tokenizer_stream.decode(new_token), end='', flush=True)
    except KeyboardInterrupt:
        print("  --control+c pressed, aborting generation--")
    print()
    del generator

# Download the model directly using the Hugging Face CLI
huggingface-cli download microsoft/Phi-4-mini-instruct-onnx --include cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/* --local-dir .

# Install the CPU package of ONNX Runtime GenAI
pip install --pre onnxruntime-genai

# Please adjust the model directory (-m) accordingly
curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/phi3-qa.py -o phi3-qa.py
python phi3-qa.py -m cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu

"""شغال

https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx
"""

!curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/phi3-qa.py -o phi3-qa.py

!python phi3-qa.py -m cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu

















































"""https://onnxruntime.ai/docs/get-started/with-python.html"""

!pip install onnxruntime

import requests

# رابط نموذج ONNX
url = "https://path/to/your/model.onnx"
# مسار المجلد لحفظ النموذج
output_path = "path/to/save/model.onnx"

# تنزيل النموذج
response = requests.get(url)
with open(output_path, "wb") as f:
    f.write(response.content)

print("Model downloaded and saved to", output_path)

import onnxruntime as ort
import numpy as np

# تحميل نموذج ONNX
onnx_model_path = "path/to/your/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# إنشاء مدخل بيانات للاستدلال
input_name = session.get_inputs()[0].name
input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, {input_name: input_data})

# طباعة النتيجة
print("Result:", result)





import onnx
onnx_model = onnx.load("fashion_mnist_model.onnx")
onnx.checker.check_model(onnx_model)



import onnx
onnx_model = onnx.load("https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx/resolve/main/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx")
onnx.checker.check_model(onnx_model)

!pip install onnx

import requests
import onnxruntime as ort
import numpy as np
import onnx
# %%

# رابط نموذج ONNX
url = "https://path/to/your/model.onnx"
# مسار المجلد لحفظ النموذج
output_path = "path/to/save/model.onnx"

# تنزيل النموذج
response = requests.get(url)
with open(output_path, "wb") as f:
    f.write(response.content)

print("Model downloaded and saved to", output_path)

# %%

# تحميل نموذج ONNX
onnx_model_path = "path/to/your/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# إنشاء مدخل بيانات للاستدلال
input_name = session.get_inputs()[0].name
input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, {input_name: input_data})

# طباعة النتيجة
print("Result:", result)

# %%

# %%

# %%
onnx_model = onnx.load("fashion_mnist_model.onnx")
onnx.checker.check_model(onnx_model)

# %%

# %%
onnx_model = onnx.load("microsoft/Phi-4-mini-instruct-onnx")
onnx.checker.check_model(onnx_model)



import requests
import onnx
import os

# URL of the ONNX model
url = "https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx/resolve/main/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"

# Define the local path where the model will be saved
output_path = "phi_4_mini_model.onnx"

# Download the model
try:
    response = requests.get(url, stream=True)
    response.raise_for_status()  # Raise an exception for HTTP errors (e.g., 404)
    with open(output_path, "wb") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    print("Model downloaded successfully to", output_path)

    # Load the model from the local path
    onnx_model = onnx.load(output_path)
    onnx.checker.check_model(onnx_model)
    print("Model loaded and checked successfully")

except requests.exceptions.RequestException as e:
    print(f"Error downloading the model: {e}")
except onnx.onnx_cpp2py_export.checker.ValidationError as e:
    print(f"Error validating the model: {e}")
except Exception as e:
      print(f"An unexpected error occured {e}")
finally:
    # Remove the downloaded model file
    if os.path.exists(output_path):
        os.remove(output_path)
        print(f"Temporary model file '{output_path}' removed.")









!huggingface-cli download microsoft/Phi-4-mini-instruct-onnx --local-dir ./phi4-mini-onnx

import onnxruntime as ort
from transformers import AutoTokenizer
import numpy as np

# 1. تحديد مسار الملفات
model_path = "./phi4-mini-onnx/model.onnx"  # مسار ملف النموذج
tokenizer_path = "./phi4-mini-onnx"         # مسار مجلد الـ tokenizer

# 2. تحميل الـ tokenizer
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

# 3. تحميل نموذج ONNX
session = ort.InferenceSession(model_path)

# 4. تحضير النص المدخل
input_text = "hi"  # النص الذي تريد معالجته
inputs = tokenizer(input_text, return_tensors="np")  # تحويل النص إلى تنسورات numpy

# 5. تشغيل الاستدلال
outputs = session.run(None, {
    "input_ids": inputs["input_ids"],
    "attention_mask": inputs["attention_mask"]
})

# 6. معالجة المخرجات
logits = outputs[0]  # المخرجات الأولية (logits)
predictions = np.argmax(logits, axis=-1)  # الحصول على التنبؤات

# 7. تحويل المخرجات إلى نص (اختياري)
output_text = tokenizer.decode(predictions[0], skip_special_tokens=True)
print("النص الناتج:", output_text)

import onnxruntime as ort
from transformers import AutoTokenizer
import numpy as np

# 1. تحديد مسار الملفات
model_path = "/content/phi4-mini-onnx/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"  # مسار ملف النموذج
tokenizer_path = "/content/phi4-mini-onnx/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4"         # مسار مجلد الـ tokenizer

# 2. تحميل الـ tokenizer
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

# 3. تحميل نموذج ONNX
session = ort.InferenceSession(model_path)

# 4. تحضير النص المدخل
input_text = "hi"  # النص الذي تريد معالجته
inputs = tokenizer(input_text, return_tensors="np")  # تحويل النص إلى تنسورات numpy

# 5. تشغيل الاستدلال
outputs = session.run(None, {
    "input_ids": inputs["input_ids"],
    "attention_mask": inputs["attention_mask"]
})

# 6. معالجة المخرجات
logits = outputs[0]  # المخرجات الأولية (logits)
predictions = np.argmax(logits, axis=-1)  # الحصول على التنبؤات

# 7. تحويل المخرجات إلى نص (اختياري)
output_text = tokenizer.decode(predictions[0], skip_special_tokens=True)
print("النص الناتج:", output_text)

"""https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx/discussions"""

import onnxruntime as ort
from transformers import AutoTokenizer
import numpy as np

# 1. تحديد مسار الملفات
model_path = "/content/phi4-mini-onnx/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
tokenizer_path = "/content/phi4-mini-onnx/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4"

# 2. تحميل الـ tokenizer
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

# 3. تحميل نموذج ONNX
session = ort.InferenceSession(model_path)

# 4. تحضير النص المدخل
input_text = "hi"
inputs = tokenizer(input_text, return_tensors="np")
input_ids = inputs["input_ids"]
attention_mask = inputs["attention_mask"]

# 5. تهيئة past_key_values (قيم ابتدائية فارغة)
num_layers = 27  # عدد الطبقات بناءً على الخطأ (0 إلى 26)
batch_size = 1   # حجم الدفعة
seq_length = input_ids.shape[1]  # طول التسلسل المدخل
hidden_size = 512  # يجب تعديل هذا بناءً على النموذج (راجع config.json للحصول على القيمة الصحيحة)

past_key_values = {}
for i in range(num_layers):
    past_key_values[f"past_key_values.{i}.key"] = np.zeros((batch_size, seq_length, hidden_size), dtype=np.float32)
    past_key_values[f"past_key_values.{i}.value"] = np.zeros((batch_size, seq_length, hidden_size), dtype=np.float32)

# 6. إعداد المدخلات للنموذج
model_inputs = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
    **past_key_values  # إضافة past_key_values إلى المدخلات
}

# 7. تشغيل الاستدلال
outputs = session.run(None, model_inputs)

# 8. معالجة المخرجات
logits = outputs[0]  # المخرجات الأولية (logits)
predictions = np.argmax(logits, axis=-1)  # الحصول على التنبؤات

# 9. تحويل المخرجات إلى نص
output_text = tokenizer.decode(predictions[0], skip_special_tokens=True)
print("النص الناتج:", output_text)

# (اختياري) تحديث past_key_values للتوليد التسلسلي
# past_key_values يتم تحديثه من مخرجات النموذج إذا كنت تريد الاستمرار في التوليد

!python /content/phi4-mini-onnx/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/configuration_phi3.py

import onnx
onnx_model = onnx.load("ag_news_model.onnx")
onnx.checker.check_model(onnx_model)

import onnx
onnx_model = onnx.load("/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx")
onnx.checker.check_model(onnx_model)

import onnxruntime as ort
import numpy as np
ort_sess = ort.InferenceSession('/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx')
outputs = ort_sess.run(None, {'input': text.numpy(),
                            'offsets':  torch.tensor([0]).numpy()})
# Print Result
result = outputs[0].argmax(axis=1)+1
print("This is a %s news" %ag_news_label[result[0]])

import numpy
import onnxruntime as rt

sess = rt.InferenceSession("/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx")
input_name = sess.get_inputs()[0].name
pred_onx = sess.run(None, {input_name: X_test.astype(numpy.float32)})[0]
print(pred_onx)

!pip install onnxruntime
!pip install numpy
!pip install torch
!pip install sklearn

import numpy
import onnxruntime as rt
import numpy as np
import torch
from sklearn.model_selection import train_test_split

# Generate dummy data for demonstration
X = np.random.rand(100, 10).astype(np.float32)  # 100 samples, 10 features
y = np.random.randint(0, 2, 100)             # 100 labels (0 or 1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Load the ONNX model
sess = rt.InferenceSession("/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx")

# Get the input name of the model
input_name = sess.get_inputs()[0].name

# Run inference with the test data
pred_onx = sess.run(None, {input_name: X_test.astype(numpy.float32)})[0]

# Print the prediction
print(pred_onx)

import numpy
import onnxruntime as rt

sess = rt.InferenceSession("/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx")
input_name = sess.get_inputs()[0].name
label_name = sess.get_outputs()[0].name
pred_onx = sess.run(
    [label_name], {input_name: X_test.astype(numpy.float32)})[0]
print(pred_onx)

import numpy
import onnxruntime as rt
import numpy as np
import torch
from sklearn.model_selection import train_test_split

# Generate dummy data for demonstration
X = np.random.rand(100, 10).astype(np.float32)  # 100 samples, 10 features
y = np.random.randint(0, 2, 100)             # 100 labels (0 or 1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Load the ONNX model
sess = rt.InferenceSession("/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx")

# Get the input name of the model
input_name = sess.get_inputs()[0].name
label_name = sess.get_outputs()[0].name

# Get all input names
input_names = [inp.name for inp in sess.get_inputs()]
print("Model Input Names:", input_names)

# Create dummy inputs for all required inputs
inputs = {}
for input_name in input_names:
    if input_name == 'input_ids':
        inputs[input_name] = X_test.astype(numpy.float32)
    elif input_name == 'attention_mask':
        inputs[input_name] = np.ones((X_test.shape[0], X_test.shape[1]), dtype=np.int64) # Example attention mask, adjust as needed
    elif input_name.startswith('past_key_values.'):
        # Determine the shape for the past_key_values (example, adjust as needed)
        num_layers = 27  # From traceback, it appears to go up to 26
        batch_size = X_test.shape[0]
        seq_length = X_test.shape[1]  # Assume sequence length based on input
        hidden_size = 3072 # Placeholder, replace with the actual hidden_size based on config.json
        num_heads = 32 #Placeholder, replace with actual number of heads based on config.json
        head_dim = hidden_size // num_heads
        if 'key' in input_name:
            inputs[input_name] = np.zeros((batch_size, num_heads, seq_length, head_dim), dtype=np.float32)
        elif 'value' in input_name:
            inputs[input_name] = np.zeros((batch_size, num_heads, seq_length, head_dim), dtype=np.float32)
        else:
            raise ValueError("Unexpected past_key_values input name", input_name)

# Run inference with the constructed input dictionary
pred_onx = sess.run([label_name], inputs)[0]

# Print the prediction
print(pred_onx)

"""aaaaaaaaaaaaaaaaaaaaaaaa"""

import numpy
import onnxruntime as rt
import numpy as np
import torch
from sklearn.model_selection import train_test_split

# Generate dummy data for demonstration
X = np.random.rand(100, 10).astype(np.float32)  # 100 samples, 10 features
y = np.random.randint(0, 2, 100)             # 100 labels (0 or 1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Load the ONNX model
sess = rt.InferenceSession("/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx")

# Get the input name of the model
input_name = sess.get_inputs()[0].name
label_name = sess.get_outputs()[0].name

# Get all input names
input_names = [inp.name for inp in sess.get_inputs()]
print("Model Input Names:", input_names)

# Create dummy inputs for all required inputs
inputs = {}
for input_name in input_names:
    if input_name == 'input_ids':
        inputs[input_name] = X_test.astype(numpy.float32)
    elif input_name == 'attention_mask':
        inputs[input_name] = np.ones((X_test.shape[0], X_test.shape[1]), dtype=np.int64) # Example attention mask, adjust as needed
    elif input_name.startswith('past_key_values.'):
        # Determine the shape for the past_key_values (example, adjust as needed)
        num_layers = 28  #  number of layers
        batch_size = X_test.shape[0]
        seq_length = 1 # Set to 1 for past_key_values
        hidden_size = 3072 # Placeholder, replace with the actual hidden_size based on config.json
        num_key_value_heads = 8 #Placeholder, replace with actual number of heads based on config.json, this was incorrectly set to 32
        head_dim = 128 # Placeholder, replace with the actual head_dim based on config.json, this was incorrectly calculated to be 96
        if 'key' in input_name:
            inputs[input_name] = np.zeros((batch_size, num_key_value_heads, seq_length, head_dim), dtype=np.float32)
        elif 'value' in input_name:
            inputs[input_name] = np.zeros((batch_size, num_key_value_heads, seq_length, head_dim), dtype=np.float32)
        else:
            raise ValueError("Unexpected past_key_values input name", input_name)

# Run inference with the constructed input dictionary
pred_onx = sess.run([label_name], inputs)[0]

# Print the prediction
print(pred_onx)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!wget https://github.com/microsoft/onnxruntime-genai/releases/download/v0.6.0/onnxruntime-genai-0.6.0-linux-x64.tar.gz





import onnxruntime as ort
import numpy as np
x, y = test_data[0][0], test_data[0][1]
ort_sess = ort.InferenceSession('/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx')
outputs = ort_sess.run(None, {'input': x.numpy()})

# Print Result
predicted, actual = classes[outputs[0][0].argmax(0)], classes[y]
print(f'Predicted: "{predicted}", Actual: "{actual}"')

"""https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx"""

!huggingface-cli download microsoft/Phi-4-mini-instruct-onnx --local-dir ./phi4-mini-onnx

# Download the model directly using the Hugging Face CLI
huggingface-cli download microsoft/Phi-4-mini-instruct-onnx --include Phi-4-mini-instruct-onnx/cpu_and_mobile/* --local-dir .

# Install the CPU package of ONNX Runtime GenAI
pip install --pre onnxruntime-genai

# Please adjust the model directory (-m) accordingly
curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/phi3-qa.py -o phi3-qa.py
python phi3-qa.py -m cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu

!huggingface-cli download microsoft/Phi-4-mini-instruct-onnx --include Phi-4-mini-instruct-onnx/cpu_and_mobile/* --local-dir .

https://huggingface.co/onnx-community/Llama-3.2-3B-Instruct-ONNX

"""https://huggingface.co/onnx-community/Llama-3.2-3B-Instruct-ONNX"""

/content/phi4-mini-onnx



!!python model-qa.py -m /content/phi4-mini-onnx/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -k 40 -p 0.95 -t 0.8 -r 1.0

!python model-qa.py -m /*{YourModelPath}*/Llama-3.2-3B-Instruct-onnx/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -k 40 -p 0.95 -t 0.8 -r 1.0



"""https://huggingface.co/onnx-community/Llama-3.2-3B-Instruct-ONNX"""

cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4

!huggingface-cli download microsoft/Phi-4-mini-instruct-onnx --include Phi-4-mini-instruct-onnx/cpu_and_mobile/* --local-dir .

!huggingface-cli download onnx-community/Llama-3.2-3B-Instruct-ONNX --include Llama-3.2-3B-Instruct-ONNX/cpu-int4-rtn-block-32-acc-level-4/cpu_and_mobile/* --local-dir .

# Commented out IPython magic to ensure Python compatibility.
# %cd content

Llama-3.2-3B-Instruct-ONNX/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4

!huggingface-cli download onnx-community/Llama-3.2-3B-Instruct-ONNX --include Llama-3.2-3B-Instruct-ONNX/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/* --local-dir .



!huggingface-cli download onnx-community/Llama-3.2-3B-Instruct-ONNX --include cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx.data --local-dir .

!huggingface-cli download onnx-community/Llama-3.2-3B-Instruct-ONNX --include cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx.data --local-dir .

!huggingface-cli download onnx-community/Llama-3.2-3B-Instruct-ONNX --include cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/* --local-dir .

"""https://huggingface.co/onnx-community/Llama-3.2-3B-Instruct-ONNX"""

!python model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -k 40 -p 0.95 -t 0.8 -r 1.0

!pip install --pre onnxruntime-genai

!python model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -k 40 -p 0.95 -t 0.8 -r 1.0

!python model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu -k 40 -p 0.95 -t 0.8 -r 1.0

!python model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu -k 40 -p 0.95 -t 0.8 -r 1.0

!python model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu -k 40 -p 0.95 -t 0.8 -r 1.0

!python model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu



!python model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4  -e cpu -k 40 -p 0.95 -t 0.8 -r 1.0 --chat_template '{input} [/INST]' --system_prompt '<s>[INST] <<SYS>>\n{args.system_prompt}\n<</SYS>>'

!python model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4  -e cpu

!python model-qa.py -h

!python model-qa.py --model_path /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4  -e cpu

!python model-qa.py --model_path /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4  -e cpu

!python model-qa.py --model_path /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template '{input}'

!python model-qa.py --model_path /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template '{input}' --system_prompt ''

!python model-qa.py --model_path /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template '<|user|>\n{input} <|end|>\n<|assistant|>'

!python model-qa.py --model_path /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template '{input}'

!python model-qa.py --model_path /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template '<|user|>\n{hi} <|end|>\n<|assistant|>'

!python model-qa.py --model_path /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template '<|user|>\n{hi} <|end|>\n<|assistant|>'
7]
17s
1
!python model-qa.py --model_path /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template '<|user|>\n{hi} <|end|>\n<|assistant|>'
Using System Prompt for LLAMA 3, if you are using LLAMA  2 please pass the argument --system_prompt '<s>[INST] <<SYS>>\n{args.system_prompt}\n<</SYS>>')
Prompt (Use quit() to exit): hi
Traceback (most recent call last):
  File "/content/model-qa.py", line 150, in <module>
    main(args)
  File "/content/model-qa.py", line 90, in main
    prompt = f'{args.chat_template.format(input=text)}'
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyError: 'hi'

import onnxruntime as ort
from transformers import AutoTokenizer
import numpy as np

model_path = "/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
tokenizer_path = "/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4"

tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
session = ort.InferenceSession(model_path)

input_text = input("Enter prompt (Use quit() to exit): ")
while input_text != "quit()":
    inputs = tokenizer(input_text, return_tensors="np")
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    num_layers = 32
    batch_size = 1
    seq_length = input_ids.shape[1]
    hidden_size = 3072  # تحقق من config.json إذا لزم الأمر

    past_key_values = {}
    for i in range(num_layers):
        past_key_values[f"past_key_values.{i}.key"] = np.zeros((batch_size, seq_length, hidden_size), dtype=np.float32)
        past_key_values[f"past_key_values.{i}.value"] = np.zeros((batch_size, seq_length, hidden_size), dtype=np.float32)

    model_inputs = {"input_ids": input_ids, "attention_mask": attention_mask, **past_key_values}
    outputs = session.run(None, model_inputs)

    logits = outputs[0]
    predictions = np.argmax(logits, axis=-1)
    output_text = tokenizer.decode(predictions[0], skip_special_tokens=True)
    print("Output:", output_text)

    input_text = input("Enter prompt (Use quit() to exit): ")

!python model-qa.py --model_path /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template '<|user|>\n{input} <|end|>\n<|assistant|>'

import onnxruntime as ort
from transformers import AutoTokenizer
import numpy as np

model_path = "/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
tokenizer_path = "/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4"

tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
session = ort.InferenceSession(model_path)

input_text = input("Enter prompt (Use quit() to exit): ")
while input_text != "quit()":
    inputs = tokenizer(input_text, return_tensors="np")
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    num_layers = 32
    batch_size = 1
    seq_length = input_ids.shape[1]
    hidden_size = 3072  # تحقق من config.json
    num_heads = 32      # تحقق من config.json
    head_dim = hidden_size // num_heads

    past_key_values = {}
    for i in range(num_layers):
        past_key_values[f"past_key_values.{i}.key"] = np.zeros((batch_size, num_heads, seq_length, head_dim), dtype=np.float32)
        past_key_values[f"past_key_values.{i}.value"] = np.zeros((batch_size, num_heads, seq_length, head_dim), dtype=np.float32)

    model_inputs = {"input_ids": input_ids, "attention_mask": attention_mask, **past_key_values}
    outputs = session.run(None, model_inputs)

    logits = outputs[0]
    predictions = np.argmax(logits, axis=-1)
    output_text = tokenizer.decode(predictions[0], skip_special_tokens=True)
    print("Output:", output_text)

    input_text = input("Enter prompt (Use quit() to exit): ")

*  <|user|>Tell me a joke<|end|><|assistant|>

import onnxruntime as ort
from transformers import AutoTokenizer
import numpy as np

model_path = "/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
tokenizer_path = "/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4"

tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
session = ort.InferenceSession(model_path)

input_text = input("Enter prompt (Use quit() to exit): ")
while input_text != "quit()":
    inputs = tokenizer(input_text, return_tensors="np")
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    num_layers = 28        # من num_hidden_layers
    batch_size = 1
    seq_length = input_ids.shape[1]
    num_key_value_heads = 8  # من num_key_value_heads
    head_dim = 128          # من head_dim

    past_key_values = {}
    for i in range(num_layers):
        past_key_values[f"past_key_values.{i}.key"] = np.zeros((batch_size, num_key_value_heads, seq_length, head_dim), dtype=np.float32)
        past_key_values[f"past_key_values.{i}.value"] = np.zeros((batch_size, num_key_value_heads, seq_length, head_dim), dtype=np.float32)

    model_inputs = {"input_ids": input_ids, "attention_mask": attention_mask, **past_key_values}
    outputs = session.run(None, model_inputs)

    logits = outputs[0]
    predictions = np.argmax(logits, axis=-1)
    output_text = tokenizer.decode(predictions[0], skip_special_tokens=True)
    print("Output:", output_text)

    input_text = input("Enter prompt (Use quit() to exit): ")





"""https://github.com/microsoft/onnxruntime-genai/tree/main

ayhgشغال
"""

import onnxruntime_genai as og

model = og.Model('cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')
tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()

# Set the max length to something sensible by default,
# since otherwise it will be set to the entire context length
search_options = {}
search_options['max_length'] = 2048
search_options['batch_size'] = 1

chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'

text = input("Input: ")
if not text:
   print("Error, input cannot be empty")
   exit

prompt = f'{chat_template.format(input=text)}'

input_tokens = tokenizer.encode(prompt)

params = og.GeneratorParams(model)
params.set_search_options(**search_options)
generator = og.Generator(model, params)

print("Output: ", end='', flush=True)

try:
   generator.append_tokens(input_tokens)
   while not generator.is_done():
     generator.generate_next_token()

     new_token = generator.get_next_tokens()[0]
     print(tokenizer_stream.decode(new_token), end='', flush=True)
except KeyboardInterrupt:
    print("  --control+c pressed, aborting generation--")

print()
del generator

إذا أردت إزالة رسالة ConnectionRefusedError، أضف os.environ["COLAB_DISABLE_DEBUGPY"] = "1" في بداية الكود.

"""https://github.com/microsoft/onnxruntime-genai/tree/main

aشغال جيد
"""

import onnxruntime_genai as og
import os


os.environ["COLAB_DISABLE_DEBUGPY"] = "1"
model = og.Model('cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')
tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()

# Set the max length to something sensible by default,
# since otherwise it will be set to the entire context length
search_options = {}
search_options['max_length'] = 2048
search_options['batch_size'] = 1

chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'

text = input("Input: ")
if not text:
   print("Error, input cannot be empty")
   exit

prompt = f'{chat_template.format(input=text)}'

input_tokens = tokenizer.encode(prompt)

params = og.GeneratorParams(model)
params.set_search_options(**search_options)
generator = og.Generator(model, params)

print("Output: ", end='', flush=True)

try:
   generator.append_tokens(input_tokens)
   while not generator.is_done():
     generator.generate_next_token()

     new_token = generator.get_next_tokens()[0]
     print(tokenizer_stream.decode(new_token), end='', flush=True)
except KeyboardInterrupt:
    print("  --control+c pressed, aborting generation--")

print()
del generator











import onnxruntime_genai as og
import argparse
import time

def main(args):
    if args.verbose: print("Loading model...")
    if args.timings:
        started_timestamp = 0
        first_token_timestamp = 0

    config = og.Config(args.model_path)
    config.clear_providers()
    if args.execution_provider != "cpu":
        if args.verbose: print(f"Setting model to {args.execution_provider}")
        config.append_provider(args.execution_provider)
    model = og.Model(config)

    if args.verbose: print("Model loaded")

    tokenizer = og.Tokenizer(model)
    tokenizer_stream = tokenizer.create_stream()
    if args.verbose: print("Tokenizer created")
    if args.verbose: print()

    search_options = {name:getattr(args, name) for name in ['do_sample', 'max_length', 'min_length', 'top_p', 'top_k', 'temperature', 'repetition_penalty'] if name in args}
    search_options['batch_size'] = 1

    if args.verbose: print(search_options)

    # Get model type
    model_type = None
    if hasattr(model, "type"):
        model_type = model.type
    else:
        import json, os

        with open(os.path.join(args.model_path, "genai_config.json"), "r") as f:
            genai_config = json.load(f)
            model_type = genai_config["model"]["type"]

    # Set chat template
    if args.chat_template:
        if args.chat_template.count('{') != 1 or args.chat_template.count('}') != 1:
            raise ValueError("Chat template must have exactly one pair of curly braces with input word in it, e.g. '<|user|>\n{input} <|end|>\n<|assistant|>'")
    else:
        if model_type.startswith("phi2") or model_type.startswith("phi3"):
            args.chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'
        elif model_type.startswith("phi4"):
            args.chat_template = '<|im_start|>user<|im_sep|>\n{input}<|im_end|>\n<|im_start|>assistant<|im_sep|>'
        elif model_type.startswith("llama"):
            args.chat_template = '<|start_header_id|>user<|end_header_id|>\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'
            print("Using Chat Template for LLAMA 3, if you are using LLAMA  2 please pass the argument --chat_template '{input} [/INST]')")
        elif model_type.startswith("qwen2"):
            args.chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'
        else:
            raise ValueError(f"Chat Template for model type {model_type} is not known. Please provide chat template using --chat_template")

    # Set system prompt
    if "<|" in args.system_prompt and "|>" in args.system_prompt:
        # User-provided system template already has tags
        system_prompt = args.system_prompt
    else:
        if model_type.startswith('phi2') or model_type.startswith('phi3'):
            system_prompt = f"<|system|>\n{args.system_prompt}<|end|>"
        elif model_type.startswith('phi4'):
            system_prompt = f"<|im_start|>system<|im_sep|>\n{args.system_prompt}<|im_end|>"
        elif model_type.startswith("llama"):
            system_prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{args.system_prompt}<|eot_id|>"
            print("Using System Prompt for LLAMA 3, if you are using LLAMA  2 please pass the argument --system_prompt '<s>[INST] <<SYS>>\\n{args.system_prompt}\\n<</SYS>>')")
        elif model_type.startswith("qwen2"):
            system_prompt = f"<|im_start|>system\n{args.system_prompt}<|im_end|>\n"
        else:
            system_prompt = args.system_prompt

    system_tokens = tokenizer.encode(system_prompt)
    system_prompt_length = len(system_tokens)

    # Keep asking for input prompts in a loop
    while True:
        text = input("Prompt (Use quit() to exit): ")
        if not text:
            print("Error, input cannot be empty")
            continue

        if text == "quit()":
            break

        if args.timings: started_timestamp = time.time()

        prompt = f'{args.chat_template.format(input=text)}'
        input_tokens = tokenizer.encode(prompt)

        params = og.GeneratorParams(model)
        params.set_search_options(**search_options)
        generator = og.Generator(model, params)
        if args.verbose: print("Generator created")

        # Append system and input tokens to the generator
        generator.append_tokens(system_tokens + input_tokens)

        if args.verbose: print("Running generation loop ...")
        if args.timings:
            first = True
            new_tokens = []

        print()
        print("Output: ", end='', flush=True)

        try:
            while not generator.is_done():
                generator.generate_next_token()
                if args.timings:
                    if first:
                        first_token_timestamp = time.time()
                        first = False

                new_token = generator.get_next_tokens()[0]
                print(tokenizer_stream.decode(new_token), end='', flush=True)
                if args.timings: new_tokens.append(new_token)
        except KeyboardInterrupt:
            print("  --control+c pressed, aborting generation--")
        print()
        print()

        # Delete the generator to free the captured graph for the next generator, if graph capture is enabled

        del generator

        if args.timings:
            prompt_time = first_token_timestamp - started_timestamp
            run_time = time.time() - first_token_timestamp
            print(f"Prompt length: {len(input_tokens)}, New tokens: {len(new_tokens)}, Time to first: {(prompt_time):.2f}s, Prompt tokens per second: {len(input_tokens)/prompt_time:.2f} tps, New tokens per second: {len(new_tokens)/run_time:.2f} tps")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS, description="End-to-end AI Question/Answer example for gen-ai")
    parser.add_argument('-m', '--model_path', type=str, required=True, help='Onnx model folder path (must contain genai_config.json and model.onnx)')
    parser.add_argument('-e', '--execution_provider', type=str, required=True, choices=["cpu", "cuda", "dml"], help="Execution provider to run ONNX model with")
    parser.add_argument('-i', '--min_length', type=int, help='Min number of tokens to generate including the prompt')
    parser.add_argument('-l', '--max_length', type=int, help='Max number of tokens to generate including the prompt')
    parser.add_argument('-ds', '--do_random_sampling', action='store_true', help='Do random sampling. When false, greedy or beam search are used to generate the output. Defaults to false')
    parser.add_argument('-p', '--top_p', type=float, help='Top p probability to sample with')
    parser.add_argument('-k', '--top_k', type=int, help='Top k tokens to sample from')
    parser.add_argument('-t', '--temperature', type=float, help='Temperature to sample with')
    parser.add_argument('-re', '--repetition_penalty', type=float, help='Repetition penalty to sample with')
    parser.add_argument('-v', '--verbose', action='store_true', default=False, help='Print verbose output and timing information. Defaults to false')
    parser.add_argument('-g', '--timings', action='store_true', default=False, help='Print timing information for each generation step. Defaults to false')
    parser.add_argument('-c', '--chat_template', type=str, default='', help='Chat template to use for the prompt. User input will be injected into {input}')
    parser.add_argument('-s', '--system_prompt', type=str, default='You are a helpful AI assistant.', help='System prompt to use for the prompt.')
    args = parser.parse_args()
    main(args)

!python /content/model-qa.py -h

!python /content/model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu

!python /content/model-chat.py -m /content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu

'<|im_start|>user<|im_sep|>\n{input}<|im_end|>\n<|im_start|>assistant<|im_sep|>'

!python /content/model-chat.py -m /content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template <|user|>{input} <|end|> <|assistant|>

!python /content/model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template '<|user|>\n{input} <|end|>\n<|assistant|>'

!python /content/model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template '<|user|>\n{hi} <|end|>\n<|assistant|>'

!python /content/model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template '<|user|>\n{input} <|end|>\n<|assistant|>'

!python /content/model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template "<|user|>\n{input} <|end|>\n<|assistant|>"

!python /content/model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template "<|user|>\\n{input} <|end|>\\n<|assistant|>"



!python /content/model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template r"<|user|>\n{input} <|end|>\n<|assistant|>"

import onnxruntime_genai as og
import argparse
import time

def main(args):
    if args.verbose: print("Loading model...")
    if args.timings:
        started_timestamp = 0
        first_token_timestamp = 0

    config = og.Config(args.model_path)
    config.clear_providers()
    if args.execution_provider != "cpu":
        if args.verbose: print(f"Setting model to {args.execution_provider}")
        config.append_provider(args.execution_provider)
    model = og.Model(config)

    if args.verbose: print("Model loaded")

    tokenizer = og.Tokenizer(model)
    tokenizer_stream = tokenizer.create_stream()
    if args.verbose: print("Tokenizer created")
    if args.verbose: print()

    search_options = {name:getattr(args, name) for name in ['do_sample', 'max_length', 'min_length', 'top_p', 'top_k', 'temperature', 'repetition_penalty'] if name in args}
    search_options['batch_size'] = 1

    if args.verbose: print(search_options)

    # Get model type
    model_type = None
    if hasattr(model, "type"):
        model_type = model.type
    else:
        import json, os

        with open(os.path.join(args.model_path, "genai_config.json"), "r") as f:
            genai_config = json.load(f)
            model_type = genai_config["model"]["type"]

    # Set chat template
    if args.chat_template:
        if args.chat_template.count('{') != 1 or args.chat_template.count('}') != 1:
            raise ValueError("Chat template must have exactly one pair of curly braces with input word in it, e.g. '<|user|>\n{input} <|end|>\n<|assistant|>'")
    else:
        if model_type.startswith("phi2") or model_type.startswith("phi3"):
            args.chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'
        elif model_type.startswith("phi4"):
            args.chat_template = '<|im_start|>user<|im_sep|>\n{input}<|im_end|>\n<|im_start|>assistant<|im_sep|>'
        elif model_type.startswith("llama"):
            args.chat_template = '<|start_header_id|>user<|end_header_id|>\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'
            print("Using Chat Template for LLAMA 3, if you are using LLAMA  2 please pass the argument --chat_template '{input} [/INST]')")
        elif model_type.startswith("qwen2"):
            args.chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'
        else:
            raise ValueError(f"Chat Template for model type {model_type} is not known. Please provide chat template using --chat_template")
    # Set system prompt
    if "<|" in args.system_prompt and "|>" in args.system_prompt:
        # User-provided system template already has tags
        system_prompt = args.system_prompt
    else:
        if model_type.startswith('phi2') or model_type.startswith('phi3'):
            system_prompt = f"<|system|>\n{args.system_prompt}<|end|>"
        elif model_type.startswith('phi4'):
            system_prompt = f"<|im_start|>system<|im_sep|>\n{args.system_prompt}<|im_end|>"
        elif model_type.startswith("llama"):
            system_prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{args.system_prompt}<|eot_id|>"
            print("Using System Prompt for LLAMA 3, if you are using LLAMA  2 please pass the argument --system_prompt '<s>[INST] <<SYS>>\\n{args.system_prompt}\\n<</SYS>>')")
        elif model_type.startswith("qwen2"):
            system_prompt = f"<|im_start|>system\n{args.system_prompt}<|im_end|>\n"
        else:
            system_prompt = args.system_prompt

    system_tokens = tokenizer.encode(system_prompt)
    system_prompt_length = len(system_tokens)

    # Keep asking for input prompts in a loop
    while True:
        text = input("Prompt (Use quit() to exit): ")
        if not text:
            print("Error, input cannot be empty")
            continue

        if text == "quit()":
            break

        if args.timings: started_timestamp = time.time()

        prompt = f'{args.chat_template.format(input=text)}'
        input_tokens = tokenizer.encode(prompt)

        params = og.GeneratorParams(model)
        params.set_search_options(**search_options)
        generator = og.Generator(model, params)
        if args.verbose: print("Generator created")

        # Append system and input tokens to the generator
        generator.append_tokens(system_tokens + input_tokens)

        if args.verbose: print("Running generation loop ...")
        if args.timings:
            first = True
            new_tokens = []

        print()
        print("Output: ", end='', flush=True)

        try:
            while not generator.is_done():
                generator.generate_next_token()
                if args.timings:
                    if first:
                        first_token_timestamp = time.time()
                        first = False

                new_token = generator.get_next_tokens()[0]
                print(tokenizer_stream.decode(new_token), end='', flush=True)
                if args.timings: new_tokens.append(new_token)
        except KeyboardInterrupt:
            print("  --control+c pressed, aborting generation--")
        print()
        print()

        # Delete the generator to free the captured graph for the next generator, if graph capture is enabled

        del generator

        if args.timings:
            prompt_time = first_token_timestamp - started_timestamp
            run_time = time.time() - first_token_timestamp
            print(f"Prompt length: {len(input_tokens)}, New tokens: {len(new_tokens)}, Time to first: {(prompt_time):.2f}s, Prompt tokens per second: {len(input_tokens)/prompt_time:.2f} tps, New tokens per second: {len(new_tokens)/run_time:.2f} tps")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS, description="End-to-end AI Question/Answer example for gen-ai")
    parser.add_argument('-m', '--model_path', type=str, required=True, help='Onnx model folder path (must contain genai_config.json and model.onnx)')
    parser.add_argument('-e', '--execution_provider', type=str, required=True, choices=["cpu", "cuda", "dml"], help="Execution provider to run ONNX model with")
    parser.add_argument('-i', '--min_length', type=int, help='Min number of tokens to generate including the prompt')
    parser.add_argument('-l', '--max_length', type=int, help='Max number of tokens to generate including the prompt')
    parser.add_argument('-ds', '--do_random_sampling', action='store_true', help='Do random sampling. When false, greedy or beam search are used to generate the output. Defaults to false')
    parser.add_argument('-p', '--top_p', type=float, help='Top p probability to sample with')

parser.add_argument('-p', '--top_p', type=float, help='Top p probability to sample with')



!python /content/a.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu --chat_template r"<|user|>\n{input} <|end|>\n<|assistant|>"

!python a.py --model_path /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu

"""ayhgشغال

https://huggingface.co/onnx-community/Llama-3.2-3B-Instruct-ONNX
"""

import onnxruntime_genai as og

model = og.Model('cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')
tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()

search_options = {'max_length': 50, 'batch_size': 1}
chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'

while True:
    text = input("Input (type 'quit' to exit): ")
    if text == "quit":
        break
    if not text:
        print("Error, input cannot be empty")
        continue

    prompt = f'{chat_template.format(input=text)}'
    input_tokens = tokenizer.encode(prompt)

    params = og.GeneratorParams(model)
    params.set_search_options(**search_options)
    generator = og.Generator(model, params)
    generator.append_tokens(input_tokens)

    print("Output: ", end='', flush=True)
    try:
        while not generator.is_done():
            generator.generate_next_token()
            new_token = generator.get_next_tokens()[0]
            print(tokenizer_stream.decode(new_token), end='', flush=True)
    except KeyboardInterrupt:
        print("  --control+c pressed, aborting generation--")
    print()
    del generator

"""https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx

phi4
"""

import onnxruntime_genai as og

model = og.Model('/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4')
tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()

search_options = {'max_length': 50, 'batch_size': 1}
chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'

while True:
    text = input("Input (type 'quit' to exit): ")
    if text == "quit":
        break
    if not text:
        print("Error, input cannot be empty")
        continue

    prompt = f'{chat_template.format(input=text)}'
    input_tokens = tokenizer.encode(prompt)

    params = og.GeneratorParams(model)
    params.set_search_options(**search_options)
    generator = og.Generator(model, params)
    generator.append_tokens(input_tokens)

    print("Output: ", end='', flush=True)
    try:
        while not generator.is_done():
            generator.generate_next_token()
            new_token = generator.get_next_tokens()[0]
            print(tokenizer_stream.decode(new_token), end='', flush=True)
    except KeyboardInterrupt:
        print("  --control+c pressed, aborting generation--")
    print()
    del generator

# Download the model directly using the Hugging Face CLI
huggingface-cli download microsoft/Phi-4-mini-instruct-onnx --include cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/* --local-dir .

# Install the CPU package of ONNX Runtime GenAI
pip install --pre onnxruntime-genai

# Please adjust the model directory (-m) accordingly
curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/phi3-qa.py -o phi3-qa.py
python phi3-qa.py -m cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu

"""شغال

https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx
"""

!curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/phi3-qa.py -o phi3-qa.py

"""/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4"""

!python phi3-qa.py -m cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -e cpu

import onnxruntime as ort
import numpy as np

# تحميل نموذج ONNX
onnx_model_path = "cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# إنشاء مدخل بيانات للاستدلال
input_name = session.get_inputs()[0].name
input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, {input_name: input_data})

# طباعة النتيجة
print("Result:", result)

for input in session.get_inputs():
    print(input.name, input.shape)

import onnxruntime as ort
import numpy as np

# تحميل نموذج ONNX
onnx_model_path = "path/to/your/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# إنشاء مدخل بيانات للاستدلال
input_names = [input.name for input in session.get_inputs()]
input_data = {
    input_names[0]: np.random.randn(1, 3, 224, 224).astype(np.float32),
    "attention_mask": np.ones((1, 224), dtype=np.int64),
    "past_key_values.0.key": np.zeros((1, 12, 1, 64), dtype=np.float32),
    "past_key_values.0.value": np.zeros((1, 12, 1, 64), dtype=np.float32),
    # إضافة المزيد من المدخلات المطلوبة هنا بنفس الطريقة
}

# إجراء الاستدلال
result = session.run(None, input_data)

# طباعة النتيجة
print("Result:", result)

import onnxruntime as ort
import numpy as np

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# إعداد المدخلات
input_names = {inp.name: inp.shape for inp in session.get_inputs()}
input_data = {
    "input_ids": np.random.randint(0, 1000, size=(1, 128)).astype(np.int64),
    "attention_mask": np.ones((1, 128)).astype(np.int64),
}

# إعداد past_key_values
for i in range(24):  # يفترض وجود 24 past_key_values
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, 8, 128, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, 8, 128, 128)).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, input_data)

# طباعة النتيجة
print("Result:", result)

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

import onnxruntime as ort
import numpy as np

# تحميل نموذج ONNX
# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# إعداد المدخلات
input_names = {inp.name: inp.shape for inp in session.get_inputs()}
input_data = {
    "input_ids": np.random.randint(0, 1000, size=(1, 128)).astype(np.int64),
    "attention_mask": np.ones((1, 224)).astype(np.int64),
}

# إعداد past_key_values
num_layers = 28  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, 8, 128, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, 8, 128, 128)).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, input_data)

# طباعة النتيجة
print("Result:", result)

"""شغال"""

import onnxruntime as ort
import numpy as np

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# إعداد المدخلات
input_names = {inp.name: inp.shape for inp in session.get_inputs()}
input_data = {
    "input_ids": np.random.randint(0, 1000, size=(1, 128)).astype(np.int64),
    "attention_mask": np.ones((1, 224)).astype(np.int64),
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = 128  # طول التسلسل السابق

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, input_data)

# طباعة النتيجة
print("Result:", result)

"""شغال"""

import onnxruntime as ort
import numpy as np

# دالة لتحويل النص إلى ID's الرموز (يمكنك استخدام مكتبة مثل Hugging Face tokenizer)
def encode_text(text):
    # استخدام مكتبة الترميز لتحويل النص إلى ID's الرموز
    # لنفترض أن هذه الدالة تُرجع ID's الرموز المقابلة للنص
    return [10, 20, 30]  # مثال لقيم ID's الرموز

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# إعداد المدخلات
input_text = "كم يساوي 1+1"
input_ids = np.array([encode_text(input_text)]).astype(np.int64)
attention_mask = np.ones(input_ids.shape).astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = 128  # طول التسلسل السابق

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, input_data)

# طباعة النتيجة
print("Result:", result)

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل النموذج والترميز
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
tokenizer = AutoTokenizer.from_pretrained("Xenova/gpt-4o")
session = ort.InferenceSession(onnx_model_path)

# إعداد المدخلات
input_text = "كم يساوي 1+1"
input_ids = tokenizer.encode(input_text, return_tensors="np")
attention_mask = np.ones(input_ids.shape, dtype=np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = 128  # طول التسلسل السابق

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128), dtype=np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128), dtype=np.float32)

# إجراء الاستدلال
result = session.run(None, input_data)

# فك ترميز النتيجة
output_text = tokenizer.decode(result[0][0], skip_special_tokens=True)
print("Result:", output_text)

import onnxruntime as ort
import numpy as np
import json

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
vocab_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/vocab.json"
merges_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/merges.txt"

with open(vocab_path, 'r') as f:
    vocab = json.load(f)

with open(merges_path, 'r') as f:
    merges = f.read().splitlines()

# دالة لتحويل النص إلى ID's الرموز باستخدام الملفات المحلية
def encode_text(text, vocab, merges):
    tokens = text.split()  # مثال بسيط لتقسيم النص إلى رموز
    input_ids = [vocab.get(token, vocab["<unk>"]) for token in tokens]
    return input_ids

# إعداد المدخلات
input_text = "كم يساوي 1+1"
input_ids = np.array([encode_text(input_text, vocab, merges)]).astype(np.int64)
attention_mask = np.ones(input_ids.shape, dtype=np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = 128  # طول التسلسل السابق

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, input_data)

# فك ترميز النتيجة (إذا كان لديك دالة لفك الترميز، استخدمها هنا)
output_text = result[0]  # هذا يعتمد على نتيجة النموذج
print("Result:", output_text)

import onnxruntime as ort
import numpy as np
from tokenizers import Tokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = Tokenizer.from_file("/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer.json")

# إعداد المدخلات
input_text = "كم يساوي 1+1"
tokens = tokenizer.encode(input_text)
input_ids = np.array(tokens.ids).reshape(1, -1).astype(np.int64)
attention_mask = np.ones(input_ids.shape, dtype=np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = 128  # طول التسلسل السابق

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, input_data)

# فك ترميز النتيجة
output_text = tokenizer.decode(result[0][0].tolist(), skip_special_tokens=True)
print("Result:", output_text)

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل النموذج والترميز
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
tokenizer = AutoTokenizer.from_pretrained("/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

session = ort.InferenceSession(onnx_model_path)

# إعداد المدخلات
input_text = "كم يساوي 1+1"
tokens = tokenizer.encode(input_text, return_tensors="np")
input_ids = np.array(tokens).reshape(1, -1).astype(np.int64)
attention_mask = np.ones(input_ids.shape, dtype=np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = 128  # طول التسلسل السابق

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, input_data)

# فك ترميز النتيجة
output_text = tokenizer.decode(result[0][0].tolist(), skip_special_tokens=True)
print("Result:", output_text)

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "كم يساوي 1+1"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = 128  # طول التسلسل السابق

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, input_data)

# فك ترميز النتيجة
output_text = tokenizer.decode(result[0][0], skip_special_tokens=True)
print("Result:", output_text)

"""شغال"""

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "كم يساوي 1+1"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = 128  # طول التسلسل السابق

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, input_data)

# فك ترميز النتيجة
output_ids = np.argmax(result[0], axis=-1)
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print("Result:", output_text)

"""ayhgشغال"""

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "how are you?"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = 128  # طول التسلسل السابق

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إجراء الاستدلال
result = session.run(None, input_data)

# فك ترميز النتيجة
output_ids = np.argmax(result[0], axis=-1)
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print("Result:", output_text)

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "who is ai?"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = 128  # طول التسلسل السابق

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إعداد المعلمات للتوليد
max_length = 50  # تعيين الحد الأقصى لعدد التوكنات للمخرجات
output_ids = []

# توليد النص تدريجيا
for _ in range(max_length):
    result = session.run(None, input_data)
    next_token_id = np.argmax(result[0], axis=-1)[0, -1]
    output_ids.append(next_token_id)

    # تحديث المدخلات للاستدلال التالي
    input_ids = np.append(input_ids, [[next_token_id]], axis=-1)
    input_data["input_ids"] = input_ids
    input_data["attention_mask"] = np.append(attention_mask, [[1]], axis=-1)

    # تحديث past_key_values بناءً على مخرجات النموذج
    for i in range(num_layers):
        input_data[f"past_key_values.{i}.key"] = result[1][i * 2]
        input_data[f"past_key_values.{i}.value"] = result[1][i * 2 + 1]

# فك ترميز النتيجة
output_text = tokenizer.decode(output_ids, skip_special_tokens=True)
print("Result:", output_text)

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "who is ai?"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = input_ids.shape[1]  # طول التسلسل المدخل

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إعداد المعلمات للتوليد
max_length = 50  # تعيين الحد الأقصى لعدد التوكنات للمخرجات
output_ids = input_ids.tolist()[0]

# توليد النص تدريجيا
for _ in range(max_length - sequence_length):
    result = session.run(None, input_data)
    next_token_id = np.argmax(result[0], axis=-1)[0, -1]
    output_ids.append(next_token_id)

    # تحديث المدخلات للاستدلال التالي
    input_ids = np.array([output_ids], dtype=np.int64)
    input_data["input_ids"] = input_ids
    input_data["attention_mask"] = np.ones(input_ids.shape, dtype=np.int64)

    # تحديث past_key_values بناءً على مخرجات النموذج
    for i in range(num_layers):
        input_data[f"past_key_values.{i}.key"] = result[1][i * 2]
        input_data[f"past_key_values.{i}.value"] = result[1][i * 2 + 1]

# فك ترميز النتيجة
output_text = tokenizer.decode(output_ids, skip_special_tokens=True)
print("Result:", output_text)

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "who is ai?"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = input_ids.shape[1]  # طول التسلسل المدخل

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إعداد المعلمات للتوليد
max_length = 50  # تعيين الحد الأقصى لعدد التوكنات للمخرجات
output_ids = input_ids.tolist()[0]

# توليد النص تدريجيا
for _ in range(max_length - sequence_length):
    result = session.run(None, input_data)
    next_token_id = np.argmax(result[0], axis=-1)[0, -1]
    output_ids.append(next_token_id)

    # تحديث المدخلات للاستدلال التالي
    input_ids = np.array([output_ids], dtype=np.int64)
    input_data["input_ids"] = input_ids
    input_data["attention_mask"] = np.ones(input_ids.shape, dtype=np.int64)

    # التحقق من تحديث past_key_values ديناميكيًا
    if len(result) > 1:
        for i in range(num_layers):
            input_data[f"past_key_values.{i}.key"] = result[1][i * 2]
            input_data[f"past_key_values.{i}.value"] = result[1][i * 2 + 1]

# فك ترميز النتيجة
output_text = tokenizer.decode(output_ids, skip_special_tokens=True)
print("Result:", output_text)

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "who is AI?"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = input_ids.shape[1]  # طول التسلسل المدخل

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إعداد المعلمات للتوليد
max_length = 50  # تعيين الحد الأقصى لعدد التوكنات للمخرجات
output_ids = input_ids.tolist()[0]

# توليد النص تدريجيا
for _ in range(max_length - sequence_length):
    result = session.run(None, input_data)
    next_token_id = np.argmax(result[0], axis=-1)[0, -1]
    output_ids.append(next_token_id)

    # تحديث المدخلات للاستدلال التالي
    input_ids = np.array([output_ids], dtype=np.int64)
    input_data["input_ids"] = input_ids
    input_data["attention_mask"] = np.ones(input_ids.shape, dtype=np.int64)

    # التحقق من تحديث past_key_values ديناميكيًا
    if len(result) > 1:
        for i in range(num_layers):
            input_data[f"past_key_values.{i}.key"] = result[1][i * 2]
            input_data[f"past_key_values.{i}.value"] = result[1][i * 2 + 1]

# فك ترميز النتيجة
output_text = tokenizer.decode(output_ids, skip_special_tokens=True)
print("Result:", output_text)

"""شغال جيد

phi4
"""

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/f/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "who is AI?"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 32  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه
sequence_length = input_ids.shape[1]  # طول التسلسل المدخل

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إعداد المعلمات للتوليد
max_length = 50  # تعيين الحد الأقصى لعدد التوكنات للمخرجات
output_ids = input_ids.tolist()[0]

# توليد النص تدريجيا
for _ in range(max_length - sequence_length):
    result = session.run(None, input_data)
    next_token_id = np.argmax(result[0], axis=-1)[0, -1]
    output_ids.append(next_token_id)

    # تحديث المدخلات للاستدلال التالي
    input_ids = np.array([output_ids], dtype=np.int64)
    input_data["input_ids"] = input_ids
    input_data["attention_mask"] = np.ones(input_ids.shape, dtype=np.int64)

    # التحقق من تحديث past_key_values ديناميكيًا إذا كانت موجودة
    if len(result) > 1 and len(result[1]) > i * 2 + 1:
        for i in range(num_layers):
            input_data[f"past_key_values.{i}.key"] = result[1][i * 2]
            input_data[f"past_key_values.{i}.value"] = result[1][i * 2 + 1]

# فك ترميز النتيجة
output_text = tokenizer.decode(output_ids, skip_special_tokens=True)
print("Result:", output_text)

"""### لو غيرت النموذج ماذا تغير ف الكود واين تجدها"""

onnx_model_path = "path/to/new/model.onnx"

import json

config_path = "path/to/new/config.json"
with open(config_path, 'r') as f:
    config = json.load(f)

num_layers = config.get("num_hidden_layers", 32)
num_attention_heads = config.get("num_attention_heads", 8)

المعلمات الخاصة بالنموذج:

عدد الطبقات (num_layers)

عدد رؤوس الانتباه (num_attention_heads)

طول التسلسل المدخل (sequence_length)

يمكنك العثور على هذه القيم في ملف تكوين النموذج الجديد (config.json). إليك مثال على كيفية استخراج هذه القيم:
import json

config_path = "path/to/new/config.json"
with open(config_path, 'r') as f:
    config = json.load(f)

num_layers = config.get("num_hidden_layers", 32)
num_attention_heads = config.get("num_attention_heads", 8)

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "who is AI?"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 28  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 24  # عدد رؤوس الانتباه
sequence_length = input_ids.shape[1]  # طول التسلسل المدخل

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إعداد المعلمات للتوليد
max_length = 50  # تعيين الحد الأقصى لعدد التوكنات للمخرجات
output_ids = input_ids.tolist()[0]

# توليد النص تدريجيا
for _ in range(max_length - sequence_length):
    result = session.run(None, input_data)
    next_token_id = np.argmax(result[0], axis=-1)[0, -1]
    output_ids.append(next_token_id)

    # تحديث المدخلات للاستدلال التالي
    input_ids = np.array([output_ids], dtype=np.int64)
    input_data["input_ids"] = input_ids
    input_data["attention_mask"] = np.ones(input_ids.shape, dtype=np.int64)

    # التحقق من تحديث past_key_values ديناميكيًا إذا كانت موجودة
    if len(result) > 1 and len(result[1]) > i * 2 + 1:
        for i in range(num_layers):
            input_data[f"past_key_values.{i}.key"] = result[1][i * 2]
            input_data[f"past_key_values.{i}.value"] = result[1][i * 2 + 1]

# فك ترميز النتيجة
output_text = tokenizer.decode(output_ids, skip_special_tokens=True)
print("Result:", output_text)

"""{
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 24,
  "num_hidden_layers": 28,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.0.dev0",
  "use_cache": true,
  "vocab_size": 128256
}

Number of Attention Heads: 24

Number of Hidden Layers: 28

Number of Key-Value Heads: 8

https://huggingface.co/onnx-community/Llama-3.2-3B-Instruct-ONNX

/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx

شغال جيد
"""

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "who is AI?"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 28  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_attention_heads = 8  # عدد رؤوس الانتباه (يجب أن تطابق التكوين)
sequence_length = input_ids.shape[1]  # طول التسلسل المدخل

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_attention_heads, sequence_length, 128)).astype(np.float32)

# إعداد المعلمات للتوليد
max_length = 50  # تعيين الحد الأقصى لعدد التوكنات للمخرجات
output_ids = input_ids.tolist()[0]

# توليد النص تدريجيا
for _ in range(max_length - sequence_length):
    result = session.run(None, input_data)
    next_token_id = np.argmax(result[0], axis=-1)[0, -1]
    output_ids.append(next_token_id)

    # تحديث المدخلات للاستدلال التالي
    input_ids = np.array([output_ids], dtype=np.int64)
    input_data["input_ids"] = input_ids
    input_data["attention_mask"] = np.ones(input_ids.shape, dtype=np.int64)

    # التحقق من تحديث past_key_values ديناميكيًا إذا كانت موجودة
    if len(result) > 1 and len(result[1]) > i * 2 + 1:
        for i in range(num_layers):
            input_data[f"past_key_values.{i}.key"] = result[1][i * 2]
            input_data[f"past_key_values.{i}.value"] = result[1][i * 2 + 1]

# فك ترميز النتيجة
output_text = tokenizer.decode(output_ids, skip_special_tokens=True)
print("Result:", output_text)







"""شغال جيد"""

import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "who is AI?"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 28  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_key_value_heads = 8  # عدد رؤوس المفتاح والقيمة (Key-Value Heads)
sequence_length = input_ids.shape[1]  # طول التسلسل المدخل

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_key_value_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_key_value_heads, sequence_length, 128)).astype(np.float32)

# إعداد المعلمات للتوليد
max_length = 50  # تعيين الحد الأقصى لعدد التوكنات للمخرجات
output_ids = input_ids.tolist()[0]

# توليد النص تدريجيا
for _ in range(max_length - sequence_length):
    result = session.run(None, input_data)
    next_token_id = np.argmax(result[0], axis=-1)[0, -1]
    output_ids.append(next_token_id)

    # تحديث المدخلات للاستدلال التالي
    input_ids = np.array([output_ids], dtype=np.int64)
    input_data["input_ids"] = input_ids
    input_data["attention_mask"] = np.ones(input_ids.shape, dtype=np.int64)

    # التحقق من تحديث past_key_values ديناميكيًا إذا كانت موجودة
    if len(result) > 1 and len(result[1]) > i * 2 + 1:
        for i in range(num_layers):
            input_data[f"past_key_values.{i}.key"] = result[1][i * 2]
            input_data[f"past_key_values.{i}.value"] = result[1][i * 2 + 1]

# فك ترميز النتيجة
output_text = tokenizer.decode(output_ids, skip_special_tokens=True)
print("Result:", output_text)









import onnxruntime as ort
import numpy as np
from transformers import AutoTokenizer

# تحميل نموذج ONNX
onnx_model_path = "/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/model.onnx"
session = ort.InferenceSession(onnx_model_path)

# تحميل الترميز من الملفات المحلية
tokenizer = AutoTokenizer.from_pretrained("/content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/")

# إعداد المدخلات
input_text = "who is python?"
tokens = tokenizer(input_text, return_tensors="np")
input_ids = tokens["input_ids"].astype(np.int64)
attention_mask = tokens["attention_mask"].astype(np.int64)

input_data = {
    "input_ids": input_ids,
    "attention_mask": attention_mask,
}

# إعداد past_key_values مع التحقق من عدد الطبقات من تكوين النموذج
num_layers = 28  # تأكد من تطابق هذا الرقم مع عدد الطبقات في النموذج الخاص بك
num_key_value_heads = 8  # عدد رؤوس المفتاح والقيمة (Key-Value Heads)
sequence_length = input_ids.shape[1]  # طول التسلسل المدخل

for i in range(num_layers):
    input_data[f"past_key_values.{i}.key"] = np.zeros((1, num_key_value_heads, sequence_length, 128)).astype(np.float32)
    input_data[f"past_key_values.{i}.value"] = np.zeros((1, num_key_value_heads, sequence_length, 128)).astype(np.float32)

# إعداد المعلمات للتوليد
max_length = 50  # تعيين الحد الأقصى لعدد التوكنات للمخرجات
output_ids = input_ids.tolist()[0]

# توليد النص تدريجيا
for _ in range(max_length - sequence_length):
    result = session.run(None, input_data)
    next_token_id = np.argmax(result[0], axis=-1)[0, -1]
    output_ids.append(next_token_id)

    # تحديث المدخلات للاستدلال التالي
    input_ids = np.array([output_ids], dtype=np.int64)
    input_data["input_ids"] = input_ids
    input_data["attention_mask"] = np.ones(input_ids.shape, dtype=np.int64)

    # التحقق من تحديث past_key_values ديناميكيًا إذا كانت موجودة
    if len(result) > 1 and len(result[1]) > i * 2 + 1:
        for i in range(num_layers):
            input_data[f"past_key_values.{i}.key"] = result[1][i * 2]
            input_data[f"past_key_values.{i}.value"] = result[1][i * 2 + 1]

# فك ترميز النتيجة
output_text = tokenizer.decode(output_ids, skip_special_tokens=True)
print("Result:", output_text)

